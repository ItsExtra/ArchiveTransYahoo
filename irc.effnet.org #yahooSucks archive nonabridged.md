[01:23] == Solanum [webchat@31.171.154.211] has joined #yahoosucks

[01:26] == sotty [webchat@ip4d1591ed.dynamic.kabel-deutschland.de] has quit [Ping timeout: 260 seconds]

[01:47] == testi [webchat@p4FF8225C.dip0.t-ipconnect.de] has quit [Quit: Page closed]

[01:49] == X-Scale [~ARM@31.22.160.25] has joined #yahoosucks

[02:05] == arrow22 [~arrow22@104.128.136.46] has joined #yahoosucks

[02:10] == Raccoon [~waywerd@194.34.133.208] has joined #yahoosucks

[02:10] <Raccoon> Team Yahoover!

[02:14] == DogsRNice [~DogsRNice@2600:1700:7480:95f0:24fb:1e0:e1c:3aa6] has joined #yahoosucks

[02:21] <Stiletto> done playing with the PG Offline trialware. Now I'm idling until a  canonical archiver script is solid to tackle (again) the private groups  I'm a member of.

[02:30] == pew [pew@2001:bc8:6005:19:28ed:636:a18:4382] has quit [Ping timeout: 252 seconds]

[02:37] == asdf [webchat@c-24-21-236-69.hsd1.or.comcast.net] has quit [Ping timeout: 260 seconds]

[02:38] == DogsRNice [~DogsRNice@2600:1700:7480:95f0:24fb:1e0:e1c:3aa6] has quit [Read error: Connection reset by peer]

[02:42] == pew [pew@2001:bc8:6005:19:e533:4f0f:1851:a72d] has joined #yahoosucks

[03:01] == qw3rty2 [~qw3rty@92.116.185.161] has joined #yahoosucks

[03:06] == tech234a [uid352403@id-352403.stonehaven.irccloud.com] has joined #yahoosucks

[03:08] == asdf [webchat@c-24-21-236-69.hsd1.or.comcast.net] has joined #yahoosucks

[03:08] == qw3rty [~qw3rty@92.116.189.5] has quit [Ping timeout: 745 seconds]

[03:11] <SketchCow> Fuckin' anyway

[03:11] <SketchCow> DO we want me to start advertising the google sheet for signing up

[03:42] == YS [webchat@172.58.87.17] has joined #yahoosucks

[03:43] == YS [webchat@172.58.87.17] has quit [Client Quit]

[03:52] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has joined #yahoosucks

[03:55] == odemgi_ [~odemgi@200116b82c162200c4f8a267c2061174.dip.versatel-1u1.de] has quit [Ping timeout: 252 seconds]

[04:00] == SynMonger [~syn@64.199.84.25] has quit [Quit: Wait, what?]

[04:01] == qw3rty [~qw3rty@92.116.130.10] has joined #yahoosucks

[04:01] == SynMonger [~syn@64.199.84.25] has joined #yahoosucks

[04:10] == asdf_ [webchat@c-24-21-236-69.hsd1.or.comcast.net] has joined #yahoosucks

[04:10] == qw3rty2 [~qw3rty@92.116.185.161] has quit [Ping timeout: 745 seconds]

[04:13] == asdf [webchat@c-24-21-236-69.hsd1.or.comcast.net] has quit [Ping timeout: 260 seconds]

[04:27] == asdf_ [webchat@c-24-21-236-69.hsd1.or.comcast.net] has quit [Ping timeout: 260 seconds]

[04:36] == asdf [webchat@c-24-21-236-69.hsd1.or.comcast.net] has joined #yahoosucks

[04:46] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has quit [Ping timeout: 252 seconds]

[04:48] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has joined #yahoosucks

[04:49] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has quit [Remote host closed the connection]

[05:07] == manjaro-u [~manjaro-u@kti.a12.18.ktis.net] has quit [Konversation terminated!]

[05:16] == tech234a [uid352403@id-352403.stonehaven.irccloud.com] has quit [Quit: Connection closed for inactivity]

[05:17] == arrow-22 [~arrow22@104.128.136.46] has joined #yahoosucks

[05:21] == arrow-22 [~arrow22@104.128.136.46] has quit [Client Quit]

[05:26] == arrow22 [~arrow22@104.128.136.46] has quit [Ping timeout: 612 seconds]

[05:47] == mode/#yahoosucks [+o SketchCow] by PurpleSym

[05:50] == W_1 [webchat@x-134-84-102-191.reshalls.umn.edu] has joined #yahoosucks

[05:58] == satoshi [~serv@201.131.87.128.wireless.tknet-ti.com.br] has joined #yahoosucks

[06:00] <satoshi> how is the archiving going on? is the bot able to join plubic groups and save everything?

[06:00] <satoshi> public

[06:08] == isiah [webchat@cpe-184-54-77-18.swo.res.rr.com] has quit [Ping timeout: 260 seconds]

[06:29] == W_1 [webchat@x-134-84-102-191.reshalls.umn.edu] has quit [Ping timeout: 260 seconds]

[06:53] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has joined #yahoosucks

[07:13] == teststsas [~akovaski@99-12-190-228.lightspeed.milwwi.sbcglobal.net] has quit [Read error: Operation timed out]

[07:50] == JAA [~JAA@144.217.81.131] has quit [Read error: Operation timed out]

[07:50] == kiska2 [james@216.21.8.63] has quit [Read error: Operation timed out]

[07:51] == dxrt [~dxrt@209.141.42.18] has quit [ZNC - http://znc.sourceforge.net]

[07:51] == dxrt [~dxrt@209.141.42.18] has joined #yahoosucks

[07:52] == mode/#yahoosucks [+o dxrt] by Fusl

[07:53] == kiska2 [james@216.21.8.63] has joined #yahoosucks

[07:53] == mode/#yahoosucks [+o kiska2] by Fusl

[07:54] == JAA [~JAA@131.ip-144-217-81.net] has joined #yahoosucks

[07:54] == mode/#yahoosucks [+o JAA] by AlsoJAA3

[07:55] == mode/#yahoosucks [+o JAA] by Fusl

[08:21] <@betamax> satoshi: hi! unfortunately all attempts to make an auto-joining bot haven't worked

[08:22] <@betamax> Yahoo's anti-abuse systems are just too effective

[08:22] <@betamax> :(

[08:23] <@betamax> however work is being done on a manual system with some automation

[08:23] <@betamax> ie: each group must be joined manually by a person, but the group to  join and the archiving of said groups will be automatic

[08:24] <@betamax> once that's up and running you'll see me begging on this channel for  people to spend time manually completing captchas to join groups :)

[08:28] <@PurpleSym> betamax: Do you have examples for the captchas they’re using? Maybe we can break them?

[08:30] <@betamax> it's google "I am not a robot", so it's probably not going to be possible

[08:31] <@betamax> and subscribing to groups via email (which doesn't need a captcha)  doesn't work either, as you get blocked* after around 5 signups, even if you wait 30 mins between each one

[08:31] <@betamax> however, by manually completing captcha's, you can get through one every 5-10 seconds or so

[08:32] <@betamax> which is much faster than any of the automated methods I tried

[08:32] <teovall> have you looked at the API for files yet? i can't figure out how to get into folders

[08:32] <@PurpleSym> Ah yeah, recaptcha is not going to be possible for us to break.

[08:33] <@betamax> teovall: have you tried https://github.com/nsapa/yahoo-group-archiver/ ?

[08:33] <teovall> for files, yahoo-groups-backup scrapes the HTML, but i'm having trouble getting it working because i can't tell when its done loading  everything through AJAX

[08:33] <@betamax> it claims to support the files, and (thanks to the great worok by nico_32_ ) basically everything else

[08:34] <@betamax> teovall: nico_32_ managed to work out how to get into folders for the links

[08:34] <@betamax> perhaps it is the same for files?

[08:35] <teovall> ohh... ok... thx

[08:37] <thuban1> folders and events endpoints should be added to the wiki page, along with groups that use them, for testing purposes

[08:38] <@betamax> the events endpoint is... interesting

[08:39] <@betamax> it's Yahoo calendar, but using data from the groups API in the GET request

[08:39] <thuban1> that... makes sense? i mean, in context

[08:39] <thuban1> anyway i'm about to go to sleep but i can comb through logs / nsapa's  fork for urls tomorrow if nobody gets to it before then

[08:40] <@betamax> I know of https://groups.yahoo.com/neo/groups/kokun_events

[08:40] <@betamax> that's a good one for events

[08:42] == asdf [webchat@c-24-21-236-69.hsd1.or.comcast.net] has quit [Ping timeout: 260 seconds]

[08:44] <thuban1> teovall: d'you have a example of a group that stores files in folders?

[08:44] <teovall> its not a public group

[08:45] <thuban1> better a private one than nothing, at least until somebody finds a public one

[08:49] <teovall> ahh.. found it... https://groups.yahoo.com/api/v2/groups/[group]/files/?sfpath=[pathURI]

[08:51] <teovall> i don't know if i should bother to keep working on yahoo-groups-backup  though since momentum seems to be converging on other scripts

[08:52] <@betamax> really? it's the one that I think is the best right now

[08:52] <teovall> AFAIK though its the only one that supports building a static HTML site from the scraped data

[08:52] <@betamax> all it needs is polls and it will have everything

[08:53] <@betamax> which other ones do you think have more momentum

[08:53] <@betamax> ?

[08:53] <teovall> do you mean yahoo-group-archiver or yahoo-groups-backup (all these names are horrible btw)

[08:53] <@betamax> ooops!

[08:54] <teovall> i've been working on yahoo-groups-backup and it only has messages and (broken) files

[08:54] <@betamax> to clarify: I was talking about yahoo-group-archiver

[08:54] <@betamax> sorry, forgot there was another similarly-named one

[08:54] <teovall> ok

[08:56] <teovall> you're working on YahooGroups-Archiver though, aren't you?

[08:57] <@betamax> teovall: I *think* the yahoo-group-archiver one supports files in folders using the JSON api

[08:57] <teovall> yes, yahoo-group-archiver, does seem to

[08:58] <@betamax> all it needs now is polls, and we'll have a canonical script

[08:59] <teovall> this is such a mess... there's so much duplicated effort

[08:59] <@betamax> yes, there is

[08:59] <@betamax> I am going to remove my script (YahooGroups-Archiver) from the wiki

[09:00] <teovall> even within yahoo-group-archiver there's two separate forks that are being concurrently developed

[09:00] <teovall> nsapa and Frankkkkk

[09:02] <@betamax> ugh, it is a pain

[09:02] <thuban1> link to frankkkkk's? (relevant features?)

[09:04] <@betamax> frankkkkk's incorporates changes from dairiki, which mostly are error-handling bugfixes

[09:04] <@betamax> view the tree of mess: https://github.com/nsapa/yahoo-group-archiver/network/members

[09:08] <thuban1> doesn't look *too* unmergeable

[09:09] <@betamax> since nico_32_ is active in this channel I am inclined to merge everything into that one

[09:24] <teovall> we should make sure to save metadata as well as just data... files for  instance it looks to only be saving the files themselves, losing the  description, owner, and creation time

[09:25] <@betamax> I'm getting a "fileinfo.json" file saved along with the files

[09:25] <@betamax> that seems to be the metadata

[09:25] <teovall> oh ok... i've just been looking at the code not running it

[09:26] <teovall> oh... i see where that's saved

[09:26] <teovall> that's good

[09:26] <@betamax> I see it also strips the JSON from the message to give a raw .eml file

[09:27] <@betamax> I think(?) you're not losing anything there, as all the other metadata  in the JSON is derived from the raw email, but I'm not sure

[09:28] <@betamax> do any of the other scripts support polls?

[09:28] <@betamax> cause I am not sure if there is a JSON api for them

[09:28] <@betamax> group will poll if anyone wants to investigate:

[09:28] <@betamax> https://groups.yahoo.com/neo/groups/relationship-poll/info

[09:29] <teovall> i hate to see too much processing being done on stuff... the archive  should be as raw as possible... that raw data can always be massaged for presentation later

[09:30] <@betamax> I agree

[09:30] <@betamax> do you think it is a trivial matter to make it not extract the raw eml and just give the JSON ?

[09:30] <@JAA> betamax, PurpleSym: Actually, there does seem to be a way to break  reCAPTCHA through the audio challenge and speech recognition. Someone  linked a Firefox addon for this recently, which requires you to click  the "I'm not a robot" checkbox and then another button in the challenge  (and funnily enough uses Google's speech recognition to break the  captcha, among some other services), but it should be possible 

[09:30] <@JAA> to automate that.

[09:31] <@JAA> This isn't a "we can do that in an hour or two" thing though.

[09:31] <@betamax> link? I'm curious how reliable this would be

[09:31] <@JAA> I haven't tried it myself, but others said it was quite reliable. Trying to find it again.

[09:32] <@JAA> https://github.com/dessant/buster

[09:32] <@betamax> teovall: by default, yahoo-group-archiver tries to recombine the emails with attachments, but you can turn it of with a flag

[09:34] <teovall> it looks like it should be pretty simple to just save the raw json instead of .eml

[09:34] <teovall> could even just save both

[09:34] <@betamax> I'd go for just the json

[09:35] <@betamax> we'll have to store all this stuff at some point, and duplication is to be avoided :)

[09:35] <teovall> stuff like this worries me... cause doing stuff half way is worse than not at all... https://github.com/nsapa/yahoo-group-archiver/blob/master/yahoo.py#L24

[09:36] <teovall> cause that's definitely not the right way to unescape character entities... and that's definitely not all of them

[09:37] <teovall> and then you're left with partially escaped and partially unescaped

[09:39] <@betamax> teovall: the sad truth is we're going to make mistakes, we're going to lose data, we're going to get corrupted data

[09:39] <@betamax> we're going to corrupt data outselves

[09:39] <@betamax> but if it works 95% of the time, it's probably good enough

[09:39] <thuban1> teovall: iirc the unescape being used here is janky because not intended for html: https://docs.python.org/2/library/xml.sax.utils.html

[09:39] <thuban1> python 3 does this with html.unescape

[09:40] <@betamax> however, if you'd like to fix that then I'm sure nico_32_ would love a PR

[09:40] <thuban1> python 2 i think has HtmlParser.unescape?

[09:41] <@betamax> JAA: OH GOODNESS IT WORKS, IT SOLVES THE CAPTCHA'S

[09:41] <@betamax> .... this is amazing

[09:41] <teovall> well, it should only unescape things that it needs to parse to gather  all the data... but its unescaping some of the data that it's saving

[09:41] <thuban1> ah yeah

[09:44] <@JAA> :-)

[09:45] <Igloo> Yes, I've used it before

[09:46] <thuban1> teovall: idk, i think it seems reasonable to unescape filenames for folders/photos/etc

[09:47] <thuban1> agree the messages themselves might be problematic (is the 'raw' encode error relevant to this?)

[09:48] <teovall> hmm... yeah, i think the only place its used on actual data is for messages

[09:48] <teovall> https://github.com/nsapa/yahoo-group-archiver/blob/master/yahoo.py#L71

[09:48] <teovall> and if we just save the raw json that won't be a problem

[09:48] <@betamax> JAA: darn, it's blocked me

[09:48] <teovall> and if everywhere else the unescaped metadata is saved somewhere

[09:49] <@betamax> "Your computer or network may be sending automated queries. To protect  our users, we can't process your request right now. For more details  visit our help page"

[09:49] <@JAA> betamax: I'm not surprised. Probably still needs a lot of IPs.

[09:51] <@betamax> yup, switching IP and it works again

[09:51] <@betamax> I'm not clued up on this, but is there a way to get a large chunk of IPs and switch between them every 2 minutes or so?

[09:52] <teovall> i don't think so.... not without money or connections at an ISP

[09:52] <@betamax> (perhaps Fusl knows about this?)

[09:53] <thuban1> teovall: actually, we shouldn't be exclusively using the /raw endpoint at all, given encoding problems

[09:53] <thuban1> https://yahoo.uservoice.com/forums/209451-us-groups/suggestions/9644478-displaying-raw-messages-is-not-8-bit-clean

[09:55] <@betamax> is there a good alternative, though?

[09:55] <thuban1> yes

[09:55] <thuban1> hit /as well as /raw and grab ygData.messageBody

[09:55] <thuban1> */ as

[09:56] <thuban1> (cf https://groups.yahoo.com/api/v1/groups/cienciaficcion/messages/89704/raw vs https://groups.yahoo.com/api/v1/groups/cienciaficcion/messages/89704/)

[09:56] <@betamax> that'll basically double the time needed to archive a group

[09:57] <@betamax> as well as halve the time before you get blocked

[09:57] <@betamax> so it would be awesome if you could add that in to yahoo-group-archiver

[09:57] <@betamax> but please add a flag to disable that behaviour if you do

[09:58] <thuban1> aye

[09:59] <Igloo> Use IPv6

[09:59] <Igloo> :P

[10:00] <teovall> ok... so with firefox, if i save the json file from the /raw endpoint and open it in a hex editor, the characters are there

[10:00] <@betamax> that's kinda what I was wondering

[10:00] <@betamax> (the IPv6 thing)

[10:00] <teovall> if you copy and paste it out of firefox though... they're just replaced with NULL

[10:01] <@betamax> I saw Fusl mentioning something about IPv6 blocks to get around Yahoo's rate limiting

[10:01] <@betamax> would this work with reCaptcha?

[10:02] <teovall> i'm pretty sure the /raw endpoint is correct... it needs to be mime  decoded to be presentable, but i think the data is there and correct

[10:02] <thuban1> teovall: oh, huh. well that works then

[10:07] <@PurpleSym> teovall: It is not. Every broken character is encoded as EF BF BD,  which decodes to U+fffd, which is the unicode replacement character  (question mark).

[10:10] <teovall> hmm

[10:12] <teovall> oof... you're right... gross

[10:13] <teovall> what yahoo did is gross, not that you're right

[10:13] <@PurpleSym> It’s ok.

[10:16] <thuban1> question from a member of the public: "Is there anyone making a list of groups with AWOL/dead admins, or is that just going to have to be word  of mouth for people looking for those fics from now on?"

[10:16] <teovall> i wouldn't want to lose the data in the email headers... but losing all non-ASCII characters is far worse

[10:17] <thuban1> is it safe to host a _list_ of such groups on eg the archiveteam wiki  page, or should i suggest people coordinate somewhere else?

[10:21] <teovall> so yeah, to get a full archive would mean hitting both endpoints

[10:22] <@betamax> thuban1: I see no problem with a lsit

[10:22] <@betamax> *list

[10:22] <@betamax> in fact, with that way, we can draw attention to it

[10:23] <@betamax> "hey people, we have a list of groups here, anyone know the admin of any of these?"

[10:23] <@betamax> go for it

[10:23] <thuban1> okay. i'm going to give the wiki page a quick facelift so i can start directing people to it

[10:35] <@betamax> JAA: re grabbing of group front pages, that's good to know

[10:36] <@betamax> I suspect I'll have to take two approaches

[10:36] <@betamax> a) an archivebot grab to get all the CSS / JS / IMG for the wayback

[10:36] <@betamax> b) an HTML-only fast-as-you-can grab to get the info needed to determine which groups should be prioritized

[10:37] <@JAA> Yeah, sounds reasonable.

[10:39] <@JAA> By the way, the reason why qwarc can only really do b) is that it  doesn't do any HTML parsing. That's normally what eats a lot of CPU time in wpull.

[10:39] <@JAA> It's basically just a dumb "fetch and store to WARC" tool which you have to tell *exactly* what to retrieve.

[10:40] <@JAA> Instead of HTML parsing, I use str.index and sometimes regex to extract the information needed to fetch the other things.

[10:40] <@betamax> that will probably work in this case

[10:41] <@betamax> I still have no idea exactly how we should be priorisising groups

[10:41] <@betamax> are oldest more important?

[10:41] <@betamax> ones with the most members? the most messages?

[10:41] <@JAA> Probably a mixture of all of these.

[10:42] <@betamax> a lot of groups filled up with spam after they became inactive, do we  ignore the ones with little activity, then no activity, then lots of  activity?

[10:42] <@betamax> and so on...

[10:42] <@betamax> I don't feel particularly qualified to make these decisions

[10:43] <@betamax> and of course, this doesn't consider public groups

[10:43] <@betamax> which could have photos / files / databases / links

[10:43] <@betamax> do we search group descriptions / names for keywords which could indicate lots of photos / files ?

[10:43] <@betamax> etc.....

[10:54] <@PurpleSym> Imo you should prioritize the groups I did not grab plus HTML/files/galleries/…

[10:55] <@PurpleSym> And I would prioritize early messages, since there was an uprise in spam around 2013/2014.

[10:56] <@betamax> what about pseudo-private groups? did you attempt to archive any of them?

[10:56] <@PurpleSym> Nope, public only.

[10:57] <@betamax> do you have a list of all the groups you grabbed?

[11:00] <@PurpleSym> Hm, not sure. That big JSON file includes private groups.

[11:00] <@betamax> how did you consider a group "public"

[11:01] <@betamax> (I should really check the fields in the JSON file you sent me before wasting your time. Sorry)

[11:01] <@PurpleSym> I did not use metadata for that. The script just tried to fetch messages and if it failed it moved on, I think.

[11:01] <@PurpleSym> But let me check…

[11:03] <@PurpleSym> Yes, it simply skipped the group if a code 1101, 1203 or 2102 error was received.

[11:04] <@betamax> Ah, OK

[11:32] == testi [webchat@p4FF8225C.dip0.t-ipconnect.de] has joined #yahoosucks

[11:35] <@PurpleSym> I could probably compile a list of groups which I have zero messages  for, if that’s useful. But it’s going to take some time (again).

[11:40] <@betamax> I'd wait for now

[11:40] <@betamax> until a clearer strategy is apparent

[11:42] <@betamax> right now, SketchCow tweeting the link to the form is bringing in responses

[11:43] <@betamax> 144 responses so far - we'll focus on these first

[11:43] <@betamax> https://tinyurl.com/savegroupsresults

[11:53] == phillipsj [~phillipsj@107-190-70-146.cpe.teksavvy.com] has joined #yahoosucks

[11:59] <@markedL> what kind of group does the email join work on? 

[12:01] <@betamax> technically all groups

[12:02] <@betamax> but you get blocked pretty quickly (ie: after 5 or so groups)

[12:02] <@betamax> and with admin-approval ones, you have to email back and explain why you want to join

[12:16] <phillipsj> so catch-all addressess of the form yahoo2MDD@domain then?

[12:18]  * phillipsj used https://www.grc.com/x/ne.dll?rgyemqde (https://www.grc.com/ppp.htm with a custom charater set)

[12:18] <@betamax> It's <groupname>-subscribe@yahoogroups.com

[12:18] <@markedL> I think he's saying we have infinite email addresses available, if you're saying we don't need a yahoo account 

[12:18] <@betamax> even if a non-english group, the .com still works

[12:18] <@betamax> wait, no we need a yahoo acount

[12:19] <@betamax> *account

[12:19] <phillipsj> oh, that sucks.

[12:19] <@betamax> because otherwise, even though you get sent new messages sent to the group

[12:19] <@betamax> you cannot login to the web interface and so cannot archive all existing data

[12:22] <@markedL> ok, that part is clear, need a yahoo account. I can think 3 of not so great but better than nothing methods for recaptcha bypass.

[12:23] <@markedL> but if the signup is the hardest part, maybe what's simplest is using  people's existing group memberships, then instead the focus is on making something that people can run on their own accounts 

[12:25] == Feathers [webchat@host-79-78-223-181.static.as9105.net] has joined #yahoosucks

[12:29] <tapedrive> ive been working on an idea betamax had

[12:30] == Feathers [webchat@host-79-78-223-181.static.as9105.net] has quit [Ping timeout: 260 seconds]

[12:30] <@PurpleSym> Also to whoever is using mongodb here (yahoo-groups-backup): Don’t. I tried that and it simply does not work.

[12:30] <@PurpleSym> (Well, not with 2 billion messages at least)

[12:31] <tapedrive> essentially think of the warrior, but for group signups, and where you need people solving captchas

[12:32] <tapedrive> its curently a tracker with a mysql db, and a chrome extension that  gets groups from there, and loads them in the page. user solves captcha, and process repeats

[12:33] <tapedrive> the thinjing is that if jason can get people to test thousands of  archive.org games/emulators, maybe we can get some peolle to solce  captchas

[12:35] <@markedL> so the 3 methods I know of for recaptcha, is distributing to humans,  using google cookies with long term history, and using the audio  recognition api's ( https://duckduckgo.com/?q=audio+recaptcha+bypass+github&t=canonical&ia=web )

[12:39] <tapedrive> can you explain "google cookies with long term histoey" a bit more?

[12:41] == mode/#yahoosucks [+o tapedrive] by betamax

[12:42] <@markedL> I'm going to paste a link so we can read the primary sources on it https://en.wikipedia.org/wiki/ReCAPTCHA#No_CAPTCHA_reCAPTCHA

[12:45] <@betamax> the issue with that is that mass-signups is pretty bot-like

[12:45] <@betamax> and therefore regardless of what "good" history you had previously,  after a certain number of joins you'll get the difficult captcha

[12:46] <@betamax> and therefore you'll need someone monitoring it

[12:46] <@betamax> meaning you might as well just have someone manually complete them in the first place

[12:50] <@markedL> I don't think anything with human involvement on our side is the easiest option. Some human involvement should be "outsourced" to the group admins or group members 

[12:52] <@betamax> I don't see any way to do it without human involvement on our side

[12:54] <@markedL> I'd propose, we build a desktop app that works on win/mac/linux, asks  for a yahoo username/password or finds a cookie in a cookie jar, runs a  crawl on their computer/network connection, and rsync or posts it to us  if they choose to an option 

[12:54] <@betamax> define "run a craw"

[12:55] <@betamax> because we already have yahoo-group-archiver

[12:55] <@betamax> which archives private groups but doesn't send them to us

[12:56] <@markedL> yes, building upon what already exists/most tested, makes it low risk and faster start 

[12:57] <@betamax> this still doesn't solve the problem of the millions of groups that  people aren't going to be members of, that are going to be lost

[12:57] <@betamax> I'm not saying we're going to get all of them, far from it

[12:58] <@betamax> but this would be - no *is* (it works) - a way to target high priority groups that would otherwise be lost

[12:59] <@markedL> the warrior project is going to get all public groups

[13:00] <@betamax> all *messages* on public groups

[13:00] <@betamax> not the files / photos / attachments / polls / calendars / links

[13:09] <@markedL> you ideally want to join all public groups, to get those additional types of data

[13:09] <@betamax> of course

[13:10] <@betamax> as well as joining all the groups that require you to be a member to  view messages, but don't require an admin to approve new member joins

[13:10] <@betamax> having people join as many groups as possible is the only was I can see to do this

[13:27] <@markedL> that's fine as an idealized goal, my input is just, don't put anything difficult in the critical path. like 3 milestones. Release something that runs on user accounts, and maybe self updates. Encourage humans to join unrepresented high priority groups. Allow drop in replacement of human's account as automated accounts improves/allows. 

[13:31] <@betamax> "Encourage humans to join unrepresented high priority groups. Allow drop in replacement of human's account"

[13:32] <@betamax> ^^ that is basically what the system I devised with tapedrive is

[13:32] <@betamax> with the addition of coordination, so that we don't get everyone joining the top 10 high priority groups

[13:33] <@betamax> literally, add the chrome extension, then it'll take you the homepage of a high priority group that hasn't been saved yet

[13:34] <@betamax> you join that one, the chrome extension detects this and gives you another

[13:34] <@betamax> and so on

[13:37] == vole-dev [~akovaski@99-12-190-228.lightspeed.milwwi.sbcglobal.net] has joined #yahoosucks

[13:38] <@markedL> sure, I didn't mean to steal your idea as much as understand how it fits in a timeline, the channel is hard to keep up with

[13:44] <@betamax> no, thanks for the feedback

[13:44] <@betamax> I admit I am nervous that we devised this whole system and I am realistically not sure if anyone will use it

[13:45] <@betamax> as I'm not sure how likely it is to expect that people provide manual help

[13:46] <@betamax> warriors, sure, but manually joining groups over and over? it is probably a big ask

[13:46] <@tapedrive> You actually can join them quite fast

[13:46] <@tapedrive> During testing I was managing to join 2 every 10 seconds

[13:47] <@betamax> ooh, that's twice as fast as I was expecting

[13:48] <@betamax> PurpleSym: how is the new discovery coming along? any rough ETA?

[13:54] <@PurpleSym> 2813657 discovered, still 400k search terms in queue.

[13:55] <@betamax> any idea how many terms were in the search queue to begin with?

[13:55] <@betamax> because that seems to be more than your previous grab already, which is good

[13:56] <@PurpleSym> No, it’s incremental. Whenever search returns more than 500 results I  add a letter to the search term and add it to the queue.

[13:58] <@betamax> that is really quite clever

[13:58] <@PurpleSym> Heh, no, it’s computer science 101 ;)

[14:11] <@betamax> PurpleSym: when does your discovery script get the group info API ?

[14:11] <@betamax> is that done as it discovers them, or does it do that later?

[14:13] <@PurpleSym> It doesn’t. That would be the 2nd step. I only save what’s returned by search.

[14:13] <@PurpleSym> Which is similar to info, but lacks a few fields.

[14:15] <@betamax> I don't suppose it has the https://6xq.net/paste/yahoo-groups-info.json.lz

[14:15] <@betamax> oops, not that

[14:16] <@betamax> the groupVis / memberVis / messageVis / etc... fields ?

[14:17] <@PurpleSym> It has:  ["adult","desc","flagBits","groupId","groupUrl","intlCode","lastPosted","members","messageVis","name","nameWithHighlightedText","order","restricted","title"]

[14:20] <@betamax> I think messageVis and restricted might be enough for what I need

[14:20] <@betamax> I will investigate, thanks

[14:21] <@PurpleSym> Yeah, if you want to know which groups’s archives can be accessed without joining, these are the important flags.

[14:22] <@betamax> lastPosted and members will be useful for prioritising groups as well

[14:39] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has quit [Quit: Leaving]

[14:39] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has joined #yahoosucks

[14:39] <odemgi> .

[14:39] == odemgi [~odemgi@200116b82c07eb001926999c7d4d1820.dip.versatel-1u1.de] has quit [Client Quit]

[14:40] == odemgi [~odemgi@2001:16b8:2c07:eb00:1926:999c:7d4d:1820] has joined #yahoosucks

[14:41] == Kirkman [webchat@97-85-178-47.static.stls.mo.charter.com] has joined #yahoosucks

[14:47] <nico_32_> hello again

[14:48] == Kirkman [webchat@97-85-178-47.static.stls.mo.charter.com] has quit [Ping timeout: 260 seconds]

[14:50]  * betamax waves

[14:59] <@betamax> nico_32_: fantastic work on yahoo-group-archiver, btw

[14:59] <@betamax> it's almost ready to be used as our canonical archiving script for non-public groups

[15:01] <nico_32_> it need someone with python experience to go over the code

[15:01] <@betamax> I took a look to see how possible it would be for polls to be added, and I couldn't find a JSON api

[15:01] <@betamax> so maybe we'll have to go without polls

[15:02] <nico_32_> https://groups.yahoo.com/api/v1/groups/tokra-resistance/polls?count=100 => "surveyId": 13081520 => https://groups.yahoo.com/api/v1/groups/tokra-resistance/polls/13081520

[15:03] <nico_32_> i don't know if we can get every poll

[15:04] <nico_32_> we don't get the total number of poll

[15:05] <@betamax> wow, I completely missed that API

[15:06] <nico_32_> we get the vote of every members

[15:06] <nico_32_> that can be an issue

[15:06] <@betamax> do we? I'm looking at a poll and can't get any info about the votes

[15:07] <@betamax> if some polls do indeed give the results of every member, can you see  if you also get that info if you go through the web UI

[15:07] <@betamax> because if it is accessable through the web UI, then it is not really a problem

[15:08] <nico_32_> https://pastebin.com/h48jwazn

[15:09] <@PurpleSym> nico_32_: Where’s the code?

[15:09] <nico_32_> PurpleSym: https://github.com/nsapa/yahoo-group-archiver/ ?

[15:10] <@betamax> nico_32_: oh, right. That.... could be an issue

[15:11] <nico_32_> probably should overwrite the 'responses' array 

[15:14] <@betamax> I'm not even sure we should be downloading that, tbh

[15:15] <@betamax> particularly with the email addresses, it could be seen as data-harvesting

[15:15] <@PurpleSym> nico_32_: I’m not sure it’s a good idea to forcefully encode rawEmail into latin1.

[15:15] <@betamax> my initial thought would just be to get the polls?count=100

[15:16] <@betamax> but then a poll without the data isn't very useful :\

[15:16] <nico_32_> yeah

[15:17] <@PurpleSym> Also /raw returns some headers which have been scrubbed from the raw  email data. You could add them, so threading information is preserved.

[15:17] <@betamax> JAA, Fusl, Kaz, kiska2, PurpleSym, SketchCow: ^^^ (re: polls)

[15:18] <@betamax> should we grab polls that could contain personal info

[15:18] <@PurpleSym> nico_32_: Here’s a script I wrote some time ago: https://6xq.net/paste/tombox.py

[15:18] <@betamax> note that polls require you to be a member of the group, so the info,  while "publicly accessible", as anyone can join, isn't immediately  piblic

[15:19] <@PurpleSym> Actually, it’s on github: https://github.com/PromyLOPh/yg2mbox

[15:19] <nico_32_> i think i should just download the attached files + dump the whole raw json

[15:20] <@PurpleSym> Probably better to do conversion to mbox in postprocessing, yes.

[15:20] <nico_32_> eml are nice for enduser

[15:20] <@betamax> PurpleSym: alternatively, if we grab the whole JSON, will we have everything to add in the headers at a later date

[15:20] <nico_32_> but the project is changing its goal

[15:20] <@betamax> nico_32_: if you are willing, I would definitely add polls into the script

[15:21] <@betamax> but we'll need to change how the flags work so you can selectively enable things

[15:21] == DogsRNice [~DogsRNice@2600:1700:7480:95f0:646f:ba30:bd1:dc1a] has joined #yahoosucks

[15:21] <@betamax> e.g: I want messages, photos and polls

[15:22] <nico_32_> it should already lets you download only photos and links

[15:22] == testi [webchat@p4FF8225C.dip0.t-ipconnect.de] has quit [Ping timeout: 260 seconds]

[15:22] <@betamax> rather than making a flag download something, can we make it *not* download the thing

[15:22] <nico_32_> -l -c tokra-resistance

[15:22] <nico_32_> logging in...

[15:23] <nico_32_> * Written 54 links from root folder

[15:23] <nico_32_> * Getting events between 20010130 and 20031027

[15:23] <nico_32_> yes it work

[15:24] <@betamax> so if you run it with no flags it gets everything, and you can then choose if you don't want something

[15:24] <nico_32_> ha

[15:26] <nico_32_> the way it work currently make it easy to add additionnal things

[15:28] <nico_32_> hu remplace store_true by store_false, change the description

[15:28] <nico_32_> and it should do what you want

[15:29] <@betamax> the error handling on the script needs a little tweaking, as well

[15:30] <@betamax> for unknown reasons (thanks Yahoo) there are occasions when the API gives you a 500 error

[15:30] <@betamax> ie: request message 811: fine, 812: 500 error, 813: fine

[15:31] <@betamax> I have only experienced this with messages, but it is entirely possible that Yahoo are broken in other places

[15:31] <@betamax> it is a shame that all the "s.get()" in the script are spread out

[15:31] <@betamax> makes it a bit more difficult to handle this

[15:35] <nico_32_> it use def yga.get_file & yga.download_file everywhere

[15:36] <@betamax> in this case (a message) it as get_json

[15:37] <@betamax> *was

[15:39] <@JAA> betamax: If the publicity of the data is obvious to the users, then I  don't have a problem with it. If it's not exposed in the web interface  or hidden somewhere unintuitive etc., then I'd say we should probably  skip it. And just in case, because the idea often gets thrown around  with things like this: no, rewriting the responses isn't happening, at  least in WARCs. If you want to create an independent dump 

[15:39] <@JAA> of the data in some reasonable format that could e.g. be the base for launching a mirror site, then do whatever.

[15:41] <@betamax> JAA: yes, that's what I thought

[15:42] <@betamax> although I'm not sure that these would have been WARC'd anyway, because you need cookies to get them and nico_32_'s script

[15:43] <@JAA> Anything HTTP can be WARCd in general. :-)

[15:43] <@JAA> Whether it's a good idea and whether the WBM will play it back is another question of course.

[15:44] == jOnAS88 [webchat@89.179.15.109.rev.sfr.net] has joined #yahoosucks

[15:44] <@betamax> I wonder if there's a way we can hack requests (python) to write a WARC every time it gets something

[15:44] <@JAA> Oh dear

[15:44] == thuban1 [~weechat@c-73-211-96-74.hsd1.il.comcast.net] has quit [Read error: Connection reset by peer]

[15:44] <@JAA> I did that with aiohttp, and it wasn't pleasant.

[15:45] <@JAA> It's probably easier to use a MITM proxy, e.g. warcprox.

[15:45] <@betamax> I mean, realistically, this isn't going in wayback

[15:45] <@betamax> seeing as you need to be logged in with cookies to get it

[15:45] <@betamax> so probably just getting the raw data is fine

[15:45] == thuban1 [~weechat@c-73-211-96-74.hsd1.il.comcast.net] has joined #yahoosucks

[15:45] <jOnAS88> OR MAYBE A YAHOO GROU¨PS EMULATOR IN THE FUTURE

[15:45] <jOnAS88> sorry

[15:45] <jOnAS88> caps

[15:46] <@JAA> I mean, as long as it writes the correct data, sure.

[15:46] <@JAA> And it can certainly be done with requests.

[15:46] <@JAA> But whatever it is, it must preserve the actual data sent over the  wire, not something with transfer encoding removed, headers normalised,  or things like that.

[15:46] <@JAA> And that's the part that gets quite tricky to hack into these libraries.

[15:47] == jOnAS88 [webchat@89.179.15.109.rev.sfr.net] has quit [Client Quit]

[15:47] == Jonas____ [webchat@89.179.15.109.rev.sfr.net] has joined #yahoosucks

[15:47] <@JAA> At least in the case of aiohttp, it meant monkey-patching private APIs, for example.

[15:47] <Jonas____> hello everybody

[15:56] <Jonas____> I'm getting HTTP 500 errors with up to date https://github.com/andrewferguson/YahooGroups-Archiver script for some groups (other are fine and I can see the stuck pages  with my browser). Is there any script that bypass this fake HTTP 500  error ? cheers

[15:57] <@betamax> ah, that's my script

[15:58] <@betamax> fyi: most effort is now on a different script, that can get files, photos, etc.. as well as messages

[15:58] <@betamax> however, bypassing a single 500 error is easy

[15:58] == girst [~girst@170.130.142.228] has quit [Quit: ZNC 1.7.3 - https://znc.in]

[15:58] <@betamax> say it is getting stuck on message 512

[15:58] <@betamax> just make a new file in the <groupname> folder called "512.json"

[15:59] <@betamax> run the script again, it will see this file and think it has already got that message, so will skip it

[16:01] <nico_32_> tiny update on nsapa/yahoo-group-archiver

[16:01] == girst [~girst@170.130.142.228] has joined #yahoosucks

[16:07] == tech234a [uid352403@id-352403.stonehaven.irccloud.com] has joined #yahoosucks

[16:29] == Lord_Nigh [Lord_Nigh@pool-108-52-174-156.phlapa.fios.verizon.net] has joined #yahoosucks

[16:30] <Lord_Nigh> I have access to a few restricted/private yahoo groups which imho need  archiving. other than manually contacting the group admins, is there  anything i can do?

[16:35] == testi [webchat@p4FF8225C.dip0.t-ipconnect.de] has joined #yahoosucks

[16:38] <Lord_Nigh> the restricted groups I have access to are: "catweasel" "Catweasel-Dev" "dungeon_tiles" "DX_Files"(private) "ET-3400" "gbdev" "msx-scc"  "OldDosOrcad" "oxyd" "speakjet" "the-linear-users-group" "UCSDPascal"  "yamahablackboxes" "YamahaDX" "YamahaDXfiles"

[16:38] <Lord_Nigh> everything else i'm subscribed to is public so i fed it to the form

[16:40] <Lord_Nigh> once I get back to my main computer, if there's a downloader utility, I'd be happy to at least locally archive those groups

[16:40] <Lord_Nigh> balrog: you know more about this, right?

[16:44] <Lord_Nigh> maybe we can get philpem to chime in, since his fork is the basis of the two main yahoo archiver forks

[16:45] <Lord_Nigh> which seem to have diverged a bit; one grabs more cookies and has  better retry support, the other correctly stores the last modified time  for everything

[16:45] == ephemer0l [~ephemer0l@all-is.organizedmagnetism.com] has joined #yahoosucks

[16:45] <Lord_Nigh> and the other one also stores databases and calendars properly

[16:47] <teovall> if someone can merge everything together so we have one canonical fork, i'd be more likely to contribute improvements and fixes

[16:47] <Lord_Nigh> i just poked philpem in a private message, we'll see what happens

[16:47] <Lord_Nigh> if he can cherry pick or merge in the changes of the two divergent forks...

[16:49] <vole-dev> betamax: If you're still looking for a way to login the script w/  username and password, it's pretty easy with the 'mechanize' library.  See my script for an example: https://github.com/vole-dev/Read-Alphabet-YahooGroups-Archive/blob/c3cd1b492a83d7e6b0722801c02a00af38e281bf/archive_group.py#L40

[16:57] <Lord_Nigh> actualy now that i check the dates, it looks those two trees are just  randomly committing stuff and not PRing it back to philpem 

[16:57] <Lord_Nigh> wild west of random code? not sure

[16:58] <Lord_Nigh> or just chaotic fast development due to impending yahoocalypse

[16:59] <Lord_Nigh> I've calmed down now.

[17:02] <balrog> Lord_Nigh: you should archive the public groups with cookies as well,  because that's the only way to get files/database/photos

[17:02] <@betamax> "chaotic fast development due to impending yahoocalypse"

[17:02] <@betamax> ^^ a very accurate representation

[17:03] <Lord_Nigh> i submitted the urls to the nominate tracker, i suppose I need to manually archive them too?

[17:03] <Lord_Nigh> the public ones

[17:03] <balrog> if they have files, photos, or attachments, yes

[17:03] <Lord_Nigh> the gbdev group is interesting since the moderators for that one went AWOL like 13 years ago

[17:03] == Jens [~jens@mimir.jensrex.dk] has joined #yahoosucks

[17:03] <@betamax> not necessarily

[17:04] <Lord_Nigh> and my membership was stuck in a pending state for over a decade

[17:04] <Lord_Nigh> but i checked today and apparently it somehow got approved

[17:04] <@betamax> for the URLs submitted to the google form, the goal will be to grab them completely if they allow non-admin approved joins

[17:04] <Lord_Nigh> or so it appears?

[17:05] <Lord_Nigh> those ones i submitted have the subtitle of "public group"

[17:05] <balrog> betamax: have you figured out a way to automate that joining?

[17:05] <@betamax> so any public groups submitted to the google form should get all files / etc... as well

[17:05] <Lord_Nigh> the ones i linked above have subtitle of "restricted group" where files are blocked but not messages, and the DX_Files group is the only one  set to "private group"

[17:06] <balrog> I believe it's like this:

[17:06] <balrog> "public group": anyone can view, anyone can join without mod approval

[17:06] <balrog> "restricted group": anyone can view, mod approval required to join

[17:06] <balrog> "private group": mod approval required to view anything

[17:06] <@betamax> balrog: kinda. tapedrive has developed a chrome extension which  automatically loads up a "high priority" group, you have to join it (and complete captcha) manually, the extension will detect that you've  joined, and give you another high priority group to join

[17:06] <balrog> for ALL the above, "anyone can view" refers ONLY to message history

[17:07] <balrog> files, photos, attachments, database, calendars ALL require joining, for ANY group, even if public

[17:07] <@betamax> soon (later today or tommorow) I'll be begging people on this channel to help out with that

[17:07] <@betamax> what balrog says is correct

[17:07] <@betamax> (there are a few weird cases where messages aren't visible even to members, but those are edge cases)

[17:10] <@betamax> balrog: there's also what I've been calling "pseudo-private" groups,  which are ones where you need to be a member to view, but you can join  without mod approval

[17:10] <@betamax> *need to be a member to view anything

[17:39] == VerifiedJ [~Joseph@host86-175-22-235.range86-175.btcentralplus.com] has joined #yahoosucks

[17:42] <Solanum> hello

[17:42] <Solanum> I was about a week into learning about this archiving stuff when this yahoo groups thing happened

[17:43] <Solanum> I am a trans man (aka FTM) and a lot of our history is on those groups

[17:43] <Solanum> they were very very important

[17:43] <Solanum> I am abit panicked trying to learn fast which doens't work

[17:45] <Solanum> All these groups are set to private

[17:45] <Solanum> Is there a page somewhere that you can reccomend me to read?

[17:45] <@betamax> Solanum: hi! thanks for stopping by

[17:46] <@betamax> by "private" do you mean that all new membership requests must be approved by an admin?

[17:46] <Solanum> Yes

[17:46] <Solanum> And some of them are inactive x years

[17:46] <Solanum> So it'll be a lot of work jsut to get into them

[17:46] <Solanum> But it's probably doable

[17:46] <@betamax> OK. Are you a member of all the groups you want to archive?

[17:46] <Solanum> Nope!

[17:47] <@betamax> right. That could be difficult.

[17:47] <Solanum> It's s mall community

[17:47] <Solanum> If I bother enough people I should be able to get in to at least some of them

[17:47] <@betamax> I would recommend trying to join them, then asking around to see if someone has already joined

[17:48] <@betamax> because it isn't strictly necessary for you to be a member, only that you can borrow the Yahoo account of someone who is

[17:48] <Solanum> Once I do that, is there a way to get the data? I was tryign to follow the chat above and it looks.. not straight forward

[17:48] <Solanum> hmm good point. they would need to trust me

[17:49] <@betamax> "chaotic fast development due to impending yahoocalypse" was one description of the work on the archival scripts right now

[17:49] <Solanum> OK maybe I'll give that a bit of time and work on getting access

[17:49] <Solanum> I have started a list https://calc.disroot.org/tijh1mt2c7v7

[17:50] <@betamax> basically, there are many tools to get just messages, and we're (or,  rather nico_32_ is) standardising on one tool to get everything from a  group

[17:50] <Solanum> I don't even know the names of all the groups. They are not a big thing now. But 10-20 years go.

[17:50] <@betamax> yes, access is the main thing

[17:50] <Solanum> Other than mesages what is there?

[17:50] <@betamax> photos / files / attachments / events / polls / links

[17:50] <Solanum> Right. Off course. 

[17:51] <Solanum> I will also need to convince people I am trustworthy. The groups are  private for ar eason, whcih is that people put very, very intimate  details fo their lives in them. 

[17:51] <Solanum> Like there are groups about genital surgery etc

[17:52] <@betamax> it is for that reason that "private" groups aren't going to be archived by us (archiveteam), although we will point you towards tools to do it  yourself

[17:52] <Solanum> Yes that's reasonable

[17:52] <@betamax> Yahoo are starting the first step towards closure on the 28th when they stop adding new content to the site

[17:52] <@betamax> there is a danger that at this point they no longer allow people to sign up to groups

[17:53] <@betamax> (the plan is for everything to eventually be invite-only)

[17:53] <Solanum> Yes I thoiught of that. i didn't know if it meant that or not. 

[17:53] <@betamax> so I would recommend getting access to all the groups you want to save by 28th Oct

[17:53] <@betamax> It's Yahoo... we're assuming the worst (although they did change it from the 21st to 28th, which is good)

[17:54] <Solanum> By the way if anyone else interested in LGBTQIAetcetc shows up, I can be reached at inthere@disroot.org 

[17:54] <Solanum> I am going to write something to send around to start on that. I wanted to know what there was by way of a plan for actually getting the data  before i started. Sounds like that is TBD at the moment

[17:55] <Solanum> Is this the best place to find updates or is there somewhere easier to catch up on?

[17:55] <@betamax> so, there is a script that can get everything (except maybe polls)

[17:55] <@betamax> but it is very flakey

[17:55] <Solanum> Can any member use it?

[17:55] <@betamax> yes, we'll have a solution at some point, hopefully mid-week at the latest

[17:56] <@betamax> https://github.com/nsapa/yahoo-group-archiver

[17:56] <@betamax> that's it there

[17:56] <Solanum> What about something like site-sucker or grab-site or wget?

[17:56] <@betamax> more difficult, as you'll need to deal with cookies and JS

[17:56] <Solanum> Those were the ones I was working on learning before I found out about this

[17:56] <Solanum> So they won't work really at all?

[17:57] <Solanum> Or will work but not ideally?

[17:57] <@betamax> they'll work for public groups, but not (without tweaking) for private groups

[17:57] <Solanum> because you need to be logged in?

[17:58] <@betamax> yes, and because some things on the site use javascript to load, which those other tools don't work as well on

[17:58] <Solanum> oh gosh

[17:59] <Solanum> I read something about using a cookies.txt file for logins for wget (or mabe grab-site) but if there is javascript for showing content.. that's a problem

[18:00] <@betamax> there may not be for messages, but I think things like images might not work as well

[18:01] <@betamax> hence the work on specialist tools just for Yahoo Groups

[18:01] <@betamax> we'll be releasing instructions on how to use the tools once they're ready

[18:01] <Solanum> I think getting anything would be better than nothing, but for the  groups I am hoping to get images are a huge component. People putt heir  surgery pics and other stuff like that. 

[18:02] <Solanum> I should watch the github linked above for thaT?

[18:03] <teovall> yes, momentum seems to be converging on that script... so that repo or  one of it's forks is likely to be the tool of choice going forward

[18:04] <Solanum> OK great

[18:04] <teovall> i'd say it's at the 80% point... 80-90% should go quick... it's that last 10% that will tough

[18:04] <Solanum> In the event that any of these groups are public, is there a list of  what has already been archived, or what is in the queue, so I don't have to spend time duplicating work?

[18:05] <@betamax> the best place to check is probably the wiki

[18:05] <Solanum> Ya there is usuallya. reason to leave certain bits til the end.. that they are extra difficult haha

[18:05] <@betamax> as we'll link to a tool and have instructions there once done

[18:06] <Solanum> this? https://www.archiveteam.org/index.php?title=Yahoo!_Groups

[18:06] <@betamax> tes

[18:06] <@betamax> *yes

[18:06] <Solanum> perfect thank you so much for taking your time

[18:06] <Solanum> <3

[18:06] <@betamax> if you find public groups, or ones that don't need member approval

[18:06] <@betamax> then we have a nomination form

[18:06] <@betamax> https://tinyurl.com/savegroups

[18:07] <@betamax> although if you want to be safe, archive it yourself as well

[18:07] <Solanum> OK noted

[18:11] == manjaro-u [~manjaro-u@kti.a12.18.ktis.net] has joined #yahoosucks

[18:11] == thuban1 has changed nick to thuban

[18:12] <nico_32_> Lord_Nigh: i have a pull request open

[18:12] <balrog> nico_32_: to which?

[18:12] <nico_32_> to my upstream philpem

[18:13] <balrog> ah I see

[18:13] <balrog> one thing we need is a configurable random-delay

[18:13] <balrog> between all requests to yahoo

[18:13] <balrog> in my experience in the past, that reduced/eliminated getting banned

[18:13] <balrog> I forget what specific parameters worked best but I think it was 1 or 2 seconds with randomness

[18:14] <Lord_Nigh> like wget --random-wait --wait=1.5 ?

[18:14] <balrog> yes, exactly

[18:15] <Jonas____> thanks @betamax that did the trick for HTTP 500 error (and thanks for  your script, it's litteraly saving synths history here)

[18:15] <teovall> this is how yahoo-groups-backup does it... https://github.com/rmcardle/yahoo-groups-backup/blob/master/yahoo_groups_backup/scraper.py#L176

[18:15] <balrog> with it set correctly I could capture large groups without getting IP banned

[18:15] <balrog> teovall: yeah I think I used yahoo_groups_backup most recently

[18:15] <@betamax> Jonas____: great, but it's probably best to re-run archival later this  week when we have a tool to do files / photos / etc..

[18:16] <thuban> <Solanum> In the event that any of these groups are public, is  there a list of what has already been archived, or what is in the queue, so I don't have to spend time duplicating work?

[18:16] <thuban> someone else also asked me this question

[18:16] <Jonas____> hi @Lord_Nigh I took care of yamahablackboxes but anyway a second copy  would be safer. ALso I'm not on DX_files so you can definitily save it !

[18:17] <thuban> there are the lists in the descriptions of the various scrapes on IA;  should I consolidate and add them to the wiki page (with a note that  they're message history only)? are there any other groups that have  already been scraped?

[18:17] == tech234a [uid352403@id-352403.stonehaven.irccloud.com] has quit [Quit: Connection closed for inactivity]

[18:17] <@betamax> those scrapes are PurpleSym's scrapes from a while ago

[18:18] == thuban [~weechat@c-73-211-96-74.hsd1.il.comcast.net]

[18:18] == realname : weechat

[18:18] == channels : #yahoosucks 

[18:18] == server  : efnet.portlane.se [Portlane EFnet Server (IPv4, IPv6 & SSL)]

[18:18] == End of WHOIS

[18:18] <@betamax> I think there should be an easier way to get a list of all of them, as  manually going through all those items could take a while

[18:18] <@betamax> also, all of those ones are public scrapes, so will almost certainly be re-done using the warrior

[18:18] <thuban> not necessarily... copy, xclip -o >>, sort | xclip -i

[18:19] <thuban> agreed, but as long as people are asking

[18:19] == girst [~girst@170.130.142.228] has quit [Quit: ZNC 1.7.5 - https://znc.in]

[18:19] == girst [~girst@170.130.142.228] has joined #yahoosucks

[18:19] <teovall> any list of stuff that's been archived should include what tool was  used and what version/commit of that tool so that if we find problems we can go back

[18:19] <@betamax> if people are asking about a specific group, get them to submit it to te google form

[18:19] <balrog> and what classes of data have been captured

[18:20] <balrog> also we should be careful not to run into a situation where someone has a large percentage of captures and loses them due to disk failure etc

[18:20] <Raccoon> hire amazon mechanical turks :)

[18:20] <@betamax> if it's a general "what are you archiving" then the answer is as many  public groups as we can find, and a small amount of high priority  pseudo-private groups

[18:20] <Raccoon> oops, scrolled up

[18:21] <@betamax> Raccoon: I assume you're talking about the captcha issue

[18:21] <Raccoon> caught me

[18:21] == manjaro-u [~manjaro-u@kti.a12.18.ktis.net] has quit [Quit: Konversation terminated!]

[18:21] <@betamax> we'll we will be essentially doing that, but with volunteers on this channel

[18:22] <@betamax> wanna sign up :)

[18:22] <thuban> betamax: i am willing to solve captchas

[18:22] <Raccoon> need to create a captcha crypto currency